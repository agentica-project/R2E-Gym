diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 00000000..7d39d850
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,61 @@
+import xarray as xr
+import time
+import psutil
+import os
+
+def get_memory_usage():
+    """Return the memory usage in MB."""
+    process = psutil.Process(os.getpid())
+    return process.memory_info().rss / 1024 / 1024
+
+# Create a small zarr dataset for testing
+import numpy as np
+import tempfile
+
+# Create a temporary zarr store
+with tempfile.TemporaryDirectory() as tmpdir:
+    # Create a larger dataset (5000x5000) with chunking
+    data = np.random.rand(5000, 5000)
+    chunks = (100, 100)
+
+    # Create a dataset with chunking
+    ds = xr.Dataset({
+        'data': (('x', 'y'), data)
+    })
+    ds = ds.chunk({'x': 100, 'y': 100})
+
+    # Save to zarr
+    zarr_path = f"{tmpdir}/test.zarr"
+    ds.to_zarr(zarr_path, mode='w')
+
+    # Open the zarr dataset
+    print("Opening zarr dataset...")
+    ds_zarr = xr.open_dataset(zarr_path, engine='zarr')
+
+    # Print initial memory usage
+    initial_memory = get_memory_usage()
+    print(f"Initial memory usage: {initial_memory:.2f} MB")
+
+    # Access the chunks attribute (this should trigger loading the entire dataset)
+    print("Accessing ds_zarr.chunks...")
+    start_time = time.time()
+    start_memory = get_memory_usage()
+    chunks_info = ds_zarr.chunks
+    end_memory = get_memory_usage()
+    end_time = time.time()
+
+    print(f"Memory usage before accessing chunks: {start_memory:.2f} MB")
+    print(f"Memory usage after accessing chunks: {end_memory:.2f} MB")
+    print(f"Memory increase: {end_memory - start_memory:.2f} MB")
+    print(f"Time to access chunks: {end_time - start_time:.2f} seconds")
+    print(f"Chunks info: {chunks_info}")
+
+    # Check if the memory usage increased significantly
+    if end_memory - start_memory > 50:  # 50 MB threshold
+        print("ISSUE CONFIRMED: Accessing chunks loaded the entire dataset into memory")
+    else:
+        print("No significant memory increase detected")
+
+    # Clean up
+    del ds_zarr
+    print("Dataset deleted")
\ No newline at end of file
diff --git a/test_issue_example.py b/test_issue_example.py
new file mode 100644
index 00000000..5ab3cc98
--- /dev/null
+++ b/test_issue_example.py
@@ -0,0 +1,46 @@
+import xarray as xr
+import time
+import psutil
+import os
+
+def get_memory_usage():
+    """Return the memory usage in MB."""
+    process = psutil.Process(os.getpid())
+    return process.memory_info().rss / 1024 / 1024
+
+# This is the example from the issue description
+url = "https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/swot_adac/FESOM/surf/fma.zarr"
+
+try:
+    print("Opening remote zarr dataset...")
+    start_memory = get_memory_usage()
+    start_time = time.time()
+
+    ds = xr.open_dataset(url, engine='zarr')
+
+    end_time = time.time()
+    end_memory = get_memory_usage()
+
+    print(f"Time to open dataset: {end_time - start_time:.2f} seconds")
+    print(f"Memory usage after opening: {end_memory - start_memory:.2f} MB")
+
+    # Access the chunks attribute
+    print("Accessing ds.chunks...")
+    start_memory = get_memory_usage()
+    start_time = time.time()
+
+    chunks_info = ds.chunks
+
+    end_time = time.time()
+    end_memory = get_memory_usage()
+
+    print(f"Time to access chunks: {end_time - start_time:.2f} seconds")
+    print(f"Memory usage after accessing chunks: {end_memory - start_memory:.2f} MB")
+    print(f"Chunks info: {chunks_info}")
+
+    # Clean up
+    del ds
+    print("Dataset deleted")
+
+except Exception as e:
+    print(f"Error: {e}")
\ No newline at end of file
diff --git a/xarray/core/common.py b/xarray/core/common.py
index 3c328f42..a01913ab 100644
--- a/xarray/core/common.py
+++ b/xarray/core/common.py
@@ -2023,6 +2023,14 @@ def get_chunksizes(
 
     chunks: dict[Any, tuple[int, ...]] = {}
     for v in variables:
+        # First check if chunk information is available in encoding
+        encoding = getattr(v, "encoding", {})
+        if "chunks" in encoding:
+            # For zarr-backed datasets, chunks in encoding are already in the right format
+            # We don't need to load the data
+            continue
+
+        # Fall back to checking data attribute (this may load data into memory)
         if hasattr(v.data, "chunks"):
             for dim, c in v.chunksizes.items():
                 if dim in chunks and c != chunks[dim]:
