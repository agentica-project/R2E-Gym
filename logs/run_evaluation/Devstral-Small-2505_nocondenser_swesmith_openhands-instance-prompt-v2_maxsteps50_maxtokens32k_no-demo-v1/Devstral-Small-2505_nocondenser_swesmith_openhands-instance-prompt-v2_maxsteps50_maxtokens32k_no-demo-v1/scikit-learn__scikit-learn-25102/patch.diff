diff --git a/debug_dtypes.py b/debug_dtypes.py
new file mode 100644
index 000000000..eb6b4149b
--- /dev/null
+++ b/debug_dtypes.py
@@ -0,0 +1,42 @@
+import numpy as np
+import pandas as pd
+from sklearn.datasets import load_iris
+from sklearn.feature_selection import SelectKBest
+from sklearn.feature_selection import chi2
+import inspect
+
+# Load dataset
+X, y = load_iris(return_X_y=True, as_frame=True)
+X = X.astype(
+   {
+       "petal width (cm)": np.float16,
+       "petal length (cm)": np.float16,
+   }
+)
+X["cat"] = y.astype("category")
+
+print("Original dtypes:")
+print(X.dtypes)
+
+# Create a custom transformer to debug the issue
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils._set_output import _SetOutputMixin
+
+class DebugTransformer(BaseEstimator, TransformerMixin, _SetOutputMixin):
+    auto_wrap_output_keys = ("transform",)
+
+    def fit(self, X, y=None):
+        return self
+
+    def transform(self, X, y=None):
+        print("Inside transform:")
+        print("  X.dtypes:", X.dtypes)
+        print("  self._sklearn_output_config:", self._sklearn_output_config)
+        return X
+
+# Test with our custom transformer
+print("\nTesting with custom transformer:")
+transformer = DebugTransformer()
+transformer.set_output(transform="pandas", dtypes="preserve")
+X_out = transformer.fit_transform(X)
+print("Output dtypes:", X_out.dtypes)
\ No newline at end of file
diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..26a4fc96d
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,46 @@
+import numpy as np
+import pandas as pd
+from sklearn.datasets import load_iris
+from sklearn.feature_selection import SelectKBest
+from sklearn.feature_selection import chi2
+import inspect
+
+# Load dataset
+X, y = load_iris(return_X_y=True, as_frame=True)
+X = X.astype(
+   {
+       "petal width (cm)": np.float16,
+       "petal length (cm)": np.float16,
+   }
+)
+X["cat"] = y.astype("category")
+
+print("Original dtypes:")
+print(X.dtypes)
+
+# Let's look at the implementation of SelectKBest
+print("\nSelectKBest class hierarchy:")
+selector = SelectKBest(chi2, k=2)
+print(inspect.getmro(type(selector)))
+
+# Check if SelectKBest has auto_wrap_output_keys
+print("\nSelectKBest auto_wrap_output_keys:", getattr(SelectKBest, "auto_wrap_output_keys", None))
+
+# Apply transformer with pandas output
+selector = SelectKBest(chi2, k=2)
+selector.set_output(transform="pandas")
+X_out = selector.fit_transform(X, y)
+print("\nOutput dtypes (without preserving dtypes):")
+print(X_out.dtypes)
+
+# Try with preserving dtypes
+try:
+    selector = SelectKBest(chi2, k=2)
+    selector.set_output(transform="pandas", dtypes="preserve")
+    X_out = selector.fit_transform(X, y)
+    print("\nOutput dtypes (with preserving dtypes):")
+    print(X_out.dtypes)
+except Exception as e:
+    print("\nError when trying to preserve dtypes:", e)
+    import traceback
+    traceback.print_exc()
\ No newline at end of file
diff --git a/sklearn/utils/_set_output.py b/sklearn/utils/_set_output.py
index 335773c6a..0c4f438c2 100644
--- a/sklearn/utils/_set_output.py
+++ b/sklearn/utils/_set_output.py
@@ -12,6 +12,8 @@ def _wrap_in_pandas_container(
     *,
     columns,
     index=None,
+    dtypes=None,
+    original_input=None,
 ):
     """Create a Pandas DataFrame.
 
@@ -36,6 +38,16 @@ def _wrap_in_pandas_container(
     index : array-like, default=None
         Index for data.
 
+    dtypes : {None, "preserve"}, default=None
+        Configure dtypes preservation.
+
+        - `None`: Dtypes are not preserved (default behavior)
+        - `"preserve"`: Preserve the dtypes of the input DataFrame for the output
+          DataFrame.
+
+    original_input : {ndarray, dataframe}, default=None
+        Original input DataFrame. Required when dtypes="preserve".
+
     Returns
     -------
     dataframe : DataFrame
@@ -44,22 +56,38 @@ def _wrap_in_pandas_container(
     if issparse(data_to_wrap):
         raise ValueError("Pandas output does not support sparse data.")
 
+    pd = check_pandas_support("Setting output container to 'pandas'")
+
+    # Get column names if callable
     if callable(columns):
         try:
-            columns = columns()
+            cols = columns()
         except Exception:
-            columns = None
-
-    pd = check_pandas_support("Setting output container to 'pandas'")
+            cols = None
+    else:
+        cols = columns
 
+    # Create or update the DataFrame
     if isinstance(data_to_wrap, pd.DataFrame):
-        if columns is not None:
-            data_to_wrap.columns = columns
+        if cols is not None:
+            data_to_wrap.columns = cols
         if index is not None:
             data_to_wrap.index = index
-        return data_to_wrap
+    else:
+        data_to_wrap = pd.DataFrame(data_to_wrap, index=index, columns=cols)
 
-    return pd.DataFrame(data_to_wrap, index=index, columns=columns)
+    # Preserve dtypes if requested
+    if dtypes == "preserve" and original_input is not None and isinstance(original_input, pd.DataFrame):
+        # Get the dtypes from the original input for the columns that are in the output
+        if cols is not None and hasattr(original_input, 'columns'):
+            # Filter to only include columns that exist in both dataframes
+            common_cols = [col for col in cols if col in original_input.columns]
+            if common_cols:
+                dtypes_dict = original_input[common_cols].dtypes.to_dict()
+                # Apply the dtypes to the output dataframe
+                data_to_wrap = data_to_wrap.astype(dtypes_dict)
+
+    return data_to_wrap
 
 
 def _get_output_config(method, estimator=None):
@@ -81,19 +109,23 @@ def _get_output_config(method, estimator=None):
 
         - "dense": specifies the dense container for `method`. This can be
           `"default"` or `"pandas"`.
+        - "dtypes": specifies whether to preserve dtypes. This can be
+          `None` or `"preserve"`.
     """
     est_sklearn_output_config = getattr(estimator, "_sklearn_output_config", {})
     if method in est_sklearn_output_config:
         dense_config = est_sklearn_output_config[method]
+        dtypes = est_sklearn_output_config.get("dtypes", None)
     else:
         dense_config = get_config()[f"{method}_output"]
+        dtypes = None
 
     if dense_config not in {"default", "pandas"}:
         raise ValueError(
             f"output config must be 'default' or 'pandas' got {dense_config}"
         )
 
-    return {"dense": dense_config}
+    return {"dense": dense_config, "dtypes": dtypes}
 
 
 def _wrap_data_with_container(method, data_to_wrap, original_input, estimator):
@@ -123,14 +155,22 @@ def _wrap_data_with_container(method, data_to_wrap, original_input, estimator):
     """
     output_config = _get_output_config(method, estimator)
 
-    if output_config["dense"] == "default" or not _auto_wrap_is_configured(estimator):
+    if output_config["dense"] == "default":
+        return data_to_wrap
+
+    # Check if estimator is configured for auto-wrapping
+    # This includes checking if the estimator has set_output method
+    if not _auto_wrap_is_configured(estimator) and not hasattr(estimator, "set_output"):
         return data_to_wrap
 
     # dense_config == "pandas"
+    dtypes = output_config.get("dtypes", None)
     return _wrap_in_pandas_container(
         data_to_wrap=data_to_wrap,
         index=getattr(original_input, "index", None),
         columns=estimator.get_feature_names_out,
+        dtypes=dtypes,
+        original_input=original_input,
     )
 
 
@@ -161,7 +201,7 @@ def _auto_wrap_is_configured(estimator):
     auto_wrap_output_keys = getattr(estimator, "_sklearn_auto_wrap_output_keys", set())
     return (
         hasattr(estimator, "get_feature_names_out")
-        and "transform" in auto_wrap_output_keys
+        and ("transform" in auto_wrap_output_keys or hasattr(estimator, "set_output"))
     )
 
 
@@ -208,7 +248,7 @@ class _SetOutputMixin:
             setattr(cls, method, wrapped_method)
 
     @available_if(_auto_wrap_is_configured)
-    def set_output(self, *, transform=None):
+    def set_output(self, *, transform=None, dtypes=None):
         """Set output container.
 
         See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
@@ -223,6 +263,13 @@ class _SetOutputMixin:
             - `"pandas"`: DataFrame output
             - `None`: Transform configuration is unchanged
 
+        dtypes : {None, "preserve"}, default=None
+            Configure dtypes preservation for pandas output.
+
+            - `None`: Dtypes are not preserved (default behavior)
+            - `"preserve"`: Preserve the dtypes of the input DataFrame for the output
+              DataFrame. Only applicable when transform="pandas".
+
         Returns
         -------
         self : estimator instance
@@ -235,6 +282,8 @@ class _SetOutputMixin:
             self._sklearn_output_config = {}
 
         self._sklearn_output_config["transform"] = transform
+        if dtypes is not None and transform == "pandas":
+            self._sklearn_output_config["dtypes"] = dtypes
         return self
 
 
diff --git a/test_with_custom_transformer.py b/test_with_custom_transformer.py
new file mode 100644
index 000000000..31a4ec1ca
--- /dev/null
+++ b/test_with_custom_transformer.py
@@ -0,0 +1,44 @@
+import numpy as np
+import pandas as pd
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils._set_output import _SetOutputMixin
+
+# Create a custom transformer with auto_wrap_output_keys set
+class CustomTransformer(BaseEstimator, TransformerMixin, _SetOutputMixin):
+    auto_wrap_output_keys = ("transform",)
+
+    def fit(self, X, y=None):
+        return self
+
+    def transform(self, X, y=None):
+        # Return a subset of columns to simulate feature selection
+        return X[['petal length (cm)', 'cat']]
+
+    def get_feature_names_out(self, input_features=None):
+        return ['petal length (cm)', 'cat']
+
+# Create a test dataset
+X = pd.DataFrame({
+    'sepal length (cm)': np.random.rand(10),
+    'sepal width (cm)': np.random.rand(10),
+    'petal length (cm)': np.random.rand(10).astype(np.float16),
+    'petal width (cm)': np.random.rand(10).astype(np.float16),
+    'cat': pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c', 'a'])
+})
+
+print("Original dtypes:")
+print(X.dtypes)
+
+# Test without preserving dtypes
+transformer = CustomTransformer()
+transformer.set_output(transform="pandas")
+X_out = transformer.fit_transform(X)
+print("\nOutput dtypes (without preserving dtypes):")
+print(X_out.dtypes)
+
+# Test with preserving dtypes
+transformer = CustomTransformer()
+transformer.set_output(transform="pandas", dtypes="preserve")
+X_out = transformer.fit_transform(X)
+print("\nOutput dtypes (with preserving dtypes):")
+print(X_out.dtypes)
\ No newline at end of file
