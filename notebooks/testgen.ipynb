{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Quick Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gcpuser/project/r2e/r2e-edits-internal\n"
     ]
    }
   ],
   "source": [
    "# set path to the parent dir\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "os.chdir(os.path.join(os.path.expanduser('~'),'project/r2e/r2e-edits-internal'))\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "2025-03-15 03:02:24,652 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n",
      "/home/gcpuser/project/r2e/r2e-edits-internal/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, Dataset\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from r2e_edits.agenthub.runtime.docker import DockerRuntime\n",
    "from r2e_edits.agenthub.environment.env import EnvArgs, RepoEnv\n",
    "from r2e_edits.agenthub.agent.agent import AgentArgs, Agent\n",
    "from r2e_edits.agenthub.trajectory.trajectory import Trajectory\n",
    "from r2e_edits.agenthub.run.testgen import runagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patch(messages):\n",
    "    \"\"\"\n",
    "    Extracts the patch content from a message list.\n",
    "\n",
    "    Assumes that the patch is contained within markers:\n",
    "      [PATCH]\n",
    "      ...\n",
    "      [/PATCH]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content = messages[1][\"content\"]\n",
    "        if \"[PATCH]\" in content and \"[/PATCH]\" in content:\n",
    "            # Extract and return the patch content between the markers\n",
    "            return content.split(\"[PATCH]\\n\")[1].split(\"\\n[/PATCH]\")[0]\n",
    "    except (IndexError, KeyError, AttributeError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def best_verif(df):\n",
    "    def topn(n):\n",
    "        # top-1 verif\n",
    "        dfmaxprob = df.groupby(\"docker_image\")[\"avg_yes_prob\"].nlargest(n).reset_index()\n",
    "        dfmaxprob_df = pd.merge(df, dfmaxprob, on=[\"docker_image\", \"avg_yes_prob\"])\n",
    "        pass_1_highest_p2p = dfmaxprob_df.groupby(\"docker_image\")[\n",
    "            \"rewards\"\n",
    "        ].max().sum() / len(df.docker_image.unique())\n",
    "\n",
    "        print(\n",
    "            f\"Pass@1 top-{n} verif: {pass_1_highest_p2p} ({len(df.docker_image.unique())})\"\n",
    "        )\n",
    "\n",
    "    topn(1)\n",
    "    topn(2)\n",
    "    topn(3)\n",
    "    topn(4)\n",
    "    topn(5)\n",
    "\n",
    "\n",
    "agent_args = AgentArgs.from_yaml(Path(\"src/r2e_edits/agenthub/train/edit-rft.yaml\"))\n",
    "\n",
    "def run_test_patch(ds, docker_image, test_patch, patch):\n",
    "    \"\"\"\n",
    "    Applies a patch in the given environment, runs a test command, and computes the predicted reward.\n",
    "\n",
    "    If the output contains the word 'resolved', the predicted reward is 1.0, otherwise 0.0.\n",
    "    The patch is undone after testing.\n",
    "    \"\"\"\n",
    "    # print(ds)\n",
    "    try:\n",
    "        env_args = EnvArgs(ds, docker_image=docker_image)\n",
    "        env = RepoEnv(env_args)\n",
    "        env.reset()\n",
    "        env.add_commands(agent_args.command_files)\n",
    "        if test_patch:\n",
    "            env.runtime.apply_patch(test_patch)\n",
    "        else:\n",
    "            return 0\n",
    "        try:\n",
    "            # Apply the patch if provided\n",
    "            if patch:  # -> check for empty patches\n",
    "                env.runtime.apply_patch(patch)\n",
    "            # Execute the test command\n",
    "            out, error_code = env.runtime.run(\n",
    "                \"execute_bash --cmd 'python3 test_issue.py -v'\"\n",
    "            )\n",
    "            # Decide reward based on output\n",
    "            pred_reward = 1.0 if \"resolved\" in out else 0.0\n",
    "            # Reverse the patch if it was applied\n",
    "        except Exception as e:\n",
    "            print(f\"Error during patch testing: {e}\")\n",
    "            pred_reward = 0.0\n",
    "        return pred_reward\n",
    "    finally:\n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_ds = load_dataset(\"r2e-edits/32b_swebv_temp08_10_patch_verifier\")\n",
    "df = (\n",
    "    rollout_ds[\"train\"]\n",
    "    .to_pandas()\n",
    "    .rename(columns={\"docker_images\": \"docker_image\"})\n",
    ")\n",
    "df[\"patch\"] = df[\"messages\"].apply(extract_patch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('docker_image', as_index=False).agg(list)\n",
    "docker_ds = rollout_ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_patches(ds_entry, test_patch, patches):\n",
    "    res = {}\n",
    "    with ProcessPoolExecutor(10) as executor:\n",
    "        futures = {executor.submit(run_test_patch, ds_entry, ds_entry['docker_image'], test_patch, patch): idx for idx, patch in enumerate(patches)}\n",
    "        for future in tqdm(as_completed(futures), total=len(patches)):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                res[futures[future]] = result\n",
    "            except Exception as e:\n",
    "                print(f\"~~~~ Error: {e}\")\n",
    "                res[futures[future]] = 0\n",
    "    return [res[idx] for idx in sorted(res.keys())]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ds = load_dataset(\"r2e-edits/swebench-verified-v1\", split=\"test\").to_pandas().set_index(\"docker_image\", drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_index(index, filter=True):\n",
    "    # Select the matching dataset entry using the provided index\n",
    "    docker_image = grouped.iloc[index][\"docker_image\"]\n",
    "    gt_rewards = grouped.iloc[index]['rewards']\n",
    "    patches = grouped.iloc[index]['patch']\n",
    "    p2p = [len(x) for x in grouped.iloc[index]['p2p_rate']]\n",
    "\n",
    "    max_p2p = max(p2p)\n",
    "    max_p2p_indices = [i for i, x in enumerate(p2p) if x == max_p2p]\n",
    "\n",
    "    if filter:\n",
    "        patches = [patches[i] for i in max_p2p_indices]\n",
    "        gt_rewards = [gt_rewards[i] for i in max_p2p_indices]\n",
    "        p2p = [p2p[i] for i in max_p2p_indices]\n",
    "\n",
    "\n",
    "    selected_ds_entry = true_ds.loc[docker_image].to_dict()\n",
    "\n",
    "    patch_string = \"\"\n",
    "    patch_string = f\"\\n\\nHere are some example patches that have been attempted:\\n\"\n",
    "    patch_string += f\"Patch #1:\\n\\n{patches[-1]}\\n\\n\"\n",
    "    patch_string += f\"Patch #2:\\n\\n{patches[-2]}\\n\\n\"\n",
    "    # patch_string += f\"Patch #3:\\n\\n{patches[-2]}\\n\\n\"\n",
    "    # patch_string += \"IMPORTANT: ANALYZE THESE PATCHES TO IDENTIFY POTENTIAL CORNER CASES YOU SHOULD ADD TO TEST_ISSUE. YOUR TEST SHOULD CORRECTLY DISAMBIGUATE BETWEEN CORRECT (GENERALIZABLE) PATCHES AND INCORRECT (NON-GENERALIZABLE) PATCHES\"\n",
    "\n",
    "\n",
    "    selected_ds_entry['problem_statement'] = selected_ds_entry['problem_statement'] + patch_string \n",
    "\n",
    "    \n",
    "\n",
    "    testgen_traj = Trajectory.load_from_model_dump_json(runagent(\n",
    "        selected_ds_entry,\n",
    "        'test_exp',\n",
    "        max_steps=30,\n",
    "        max_steps_absolute=40,\n",
    "        llm_name=\"vertex_ai/claude-3-5-sonnet-v2@20241022\"\n",
    "    ))\n",
    "\n",
    "    print(testgen_traj.output_patch)\n",
    "\n",
    "\n",
    "    rewards = run_all_patches(selected_ds_entry, testgen_traj.output_patch, patches)\n",
    "\n",
    "    print(f\"GT rewards: {gt_rewards}\")\n",
    "    print(f\"Predicted rewards: {rewards}\")\n",
    "    return testgen_traj\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_docker_name(docker_name):\n",
    "    index = grouped[grouped['docker_image'] == docker_name].index[0]\n",
    "    run_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_docker_name(\"slimshetty/swebench-verified:sweb.eval.x86_64.pytest-dev__pytest-5631\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ds.test_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_num_files(patch):\n",
    "    ## count number of diff --git {file} {file} in a patch for unique file\n",
    "    return len(set([line.split()[2][2:] for line in patch.split('\\n') if line.startswith('diff --git')]))\n",
    "true_ds['test_num_files'] = true_ds['test_patch'].apply(count_num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ds['test_num_files'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_ds.test_patch['slimshetty/swebench-verified:sweb.eval.x86_64.pytest-dev__pytest-5631'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = grouped[grouped['docker_image'] == \"slimshetty/swebench-verified:sweb.eval.x86_64.pytest-dev__pytest-5631\"].index[0]\n",
    "\n",
    "docker_image = grouped.iloc[index][\"docker_image\"]\n",
    "gt_rewards = grouped.iloc[index]['rewards']\n",
    "patches = grouped.iloc[index]['patch']\n",
    "p2p = [len(x) for x in grouped.iloc[index]['p2p_rate']]\n",
    "\n",
    "max_p2p = max(p2p)\n",
    "max_p2p_indices = [i for i, x in enumerate(p2p) if x == max_p2p]\n",
    "\n",
    "if filter:\n",
    "    patches = [patches[i] for i in max_p2p_indices]\n",
    "    gt_rewards = [gt_rewards[i] for i in max_p2p_indices]\n",
    "    p2p = [p2p[i] for i in max_p2p_indices]\n",
    "\n",
    "print(patches[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patches[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
